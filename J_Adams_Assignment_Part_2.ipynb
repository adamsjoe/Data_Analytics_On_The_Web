{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "J_Adams_Assignment_Part_2--.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2YM-cY3XAb1"
      },
      "source": [
        "# Training a linear regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4OkOIjgXImQ"
      },
      "source": [
        "### Background\n",
        "\n",
        "The dataset which was created for part 1 has been constructed so that each borough can be identified.  This means that a linear regressor could be made for each indivual borough.  As the code uses the dataset with the same structure, what works for one, would work for all.  However, in order to simplifiy this notebook, only one borough will be used.  That borough is Manhatten.\n",
        "\n",
        "To begin with, the data must first be loaded.  Once loaded, using the query method of the pandas dataframe the rows which are in Mahattan can be extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUkPRKJw1mm6"
      },
      "source": [
        "# import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# load the data as per normal\n",
        "# df = pd.read_csv('https://raw.githubusercontent.com/adamsjoe/Data_Analytics_On_The_Web/main/Final_Data/Final_Data_Collated.csv', index_col=0)\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/adamsjoe/Data_Analytics_On_The_Web/main/Final_Data/Final_Data_Collated.csv')\n",
        "\n",
        "# now setup to query the dataframe for only the boroughs noted\n",
        "manhat_df = df.query('BOROUGH==\"MANHATTAN\"')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYJbRFX5YExF"
      },
      "source": [
        "#### Data Verification\n",
        "\n",
        "Once the data has been loaded, it is a good idea to check this.  Printing only 6 rows shows that all are Manhattan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNzPMAL314nG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4588c697-01d6-419a-ddab-87d20a412869"
      },
      "source": [
        "# check the data had loaded by printing it\n",
        "print(manhat_df[:6])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          DATE    BOROUGH  WEEKDAY  ...  PERS_KILL  PERS_INJD  NUM_COLS\n",
            "2   01-01-2017  MANHATTAN        7  ...          0         24        84\n",
            "8   01-02-2017  MANHATTAN        3  ...          0         23       135\n",
            "13  01-03-2017  MANHATTAN        3  ...          0         20       120\n",
            "17  01-04-2017  MANHATTAN        6  ...          0         14       109\n",
            "22  01-05-2017  MANHATTAN        1  ...          0         28       119\n",
            "26  01-06-2017  MANHATTAN        4  ...          0         31       141\n",
            "\n",
            "[6 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSW7NtK4dBAj"
      },
      "source": [
        "## Import numpy and shuffle\n",
        "\n",
        "Create a new shuffled data by using the random permutation function of numpy using the length of the original mahattan data frame.  In this particular data set this is important as the data is time series (or time series derived) and there could be patterns within the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-NlRVds2BEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b460af9-ca32-4dc6-916a-398d6e7974a8"
      },
      "source": [
        "import numpy as np\n",
        "shuffle_manhatten = manhat_df.iloc[np.random.permutation(len(manhat_df))]\n",
        "print(shuffle_manhatten[:5])\n",
        "\n",
        "# setup constant for use later\n",
        "SCALE_NUM_COLS = 1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             DATE    BOROUGH  WEEKDAY  ...  PERS_KILL  PERS_INJD  NUM_COLS\n",
            "16786  29-04-2017  MANHATTAN        6  ...          0         20       124\n",
            "15223  2021-03-05  MANHATTAN        5  ...          0         20        48\n",
            "1580   2012-09-15  MANHATTAN        6  ...          0         23       139\n",
            "7971   2016-03-16  MANHATTAN        3  ...          0         42       139\n",
            "9144   2016-11-05  MANHATTAN        6  ...          0         37       133\n",
            "\n",
            "[5 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFG25By8dtfc"
      },
      "source": [
        "## Predictors, Training set and Testing set\n",
        "\n",
        "Firstly the predictors are created.  These are created from the shuffled data and only the third, fourth, fifth, seventh and last column is used.  These are, in order, year, month, day, temp and num_cols (number of collisions).  If the data was not being filtered by borough at the start, then the borough would need to be brought in also.\n",
        "\n",
        "The Target is then defined - this is the last column in the shuffled dataset.\n",
        "\n",
        "The training set will be 80% of the full data set (0.8) and the testing data will be the remainder - so in this case, 100 - 80 = 20%\n",
        "\n",
        "Constants are setup for the number of predictors (3 in this case: year, month and day) and the number of outputs (or targets, which is 1 here.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T01AcNdrBh3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12874179-7c19-40f4-e096-228c435da945"
      },
      "source": [
        "# select the day, month, year and number of collisions columns.  \n",
        "predictors_manhatten = shuffle_manhatten.iloc[:,[3,4,5]]\n",
        "print(predictors_manhatten[:6])\n",
        "\n",
        "# We want the last column (the NUM_COLS)\n",
        "targets_manhattan = shuffle_manhatten.iloc[:,-1]\n",
        "print(targets_manhattan[:6])\n",
        "\n",
        "# split data into training set\n",
        "training_size_manhattan = int(len(shuffle_manhatten['NUM_COLS']) * 0.8)\n",
        "\n",
        "# test size is the size of the data - the training size (in this case 20%)\n",
        "testing_size_manhattan = len(shuffle_manhatten['NUM_COLS']) - training_size_manhattan\n",
        "\n",
        "# define the number of input params, day, month and year = 3 (predictors)\n",
        "NO_PREDICTORS_MANHATTAN = 3\n",
        "\n",
        "# define the number of output params, collisions = 1 (targets)\n",
        "NO_TARGETS = 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       YEAR  MONTH  DAY\n",
            "16786  2017      4   29\n",
            "15223  2021      3    5\n",
            "1580   2012      9   15\n",
            "7971   2016      3   16\n",
            "9144   2016     11    5\n",
            "4642   2014      5   20\n",
            "16786    124\n",
            "15223     48\n",
            "1580     139\n",
            "7971     139\n",
            "9144     133\n",
            "4642     139\n",
            "Name: NUM_COLS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9cyW1wjfAIk"
      },
      "source": [
        "### Verification\n",
        "\n",
        "Just a simple print to ensure the predictor and values are populated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYOnh5yUElQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d47833-db42-4be4-dcf2-c890600688f5"
      },
      "source": [
        "print(predictors_manhatten.values)\n",
        "print(predictors_manhatten)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2017    4   29]\n",
            " [2021    3    5]\n",
            " [2012    9   15]\n",
            " ...\n",
            " [2016    3    7]\n",
            " [2013   10   22]\n",
            " [2018    9   13]]\n",
            "       YEAR  MONTH  DAY\n",
            "16786  2017      4   29\n",
            "15223  2021      3    5\n",
            "1580   2012      9   15\n",
            "7971   2016      3   16\n",
            "9144   2016     11    5\n",
            "...     ...    ...  ...\n",
            "12755  2019     10   29\n",
            "3447   2013      9   23\n",
            "7925   2016      3    7\n",
            "3592   2013     10   22\n",
            "10701  2018      9   13\n",
            "\n",
            "[3384 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axySTDY1fKHw"
      },
      "source": [
        "## The Tensorflow bit.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIj_sLztBlsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a247b9cf-e1e8-4589-d975-f36e5751577c"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# check tensor version\n",
        "print(tf.__version__)\n",
        "\n",
        "import shutil\n",
        "\n",
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# setup some variables to hold the file path\n",
        "manhattan_dir = '/tmp/linear_regression_trained_model'\n",
        "\n",
        "# remove the last training model\n",
        "shutil.rmtree(manhattan_dir, ignore_errors=True)\n",
        "\n",
        "# estimators for each borough\n",
        "estimator_manhattan = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(\n",
        "    model_dir=manhattan_dir, \n",
        "    optimizer=tf.train.AdamOptimizer(learning_rate=0.1), \n",
        "    enable_centered_bias=False, \n",
        "    feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors_manhatten.values)\n",
        "    )\n",
        ")\n",
        "\n",
        "# # Prints a log to show model is starting to train\n",
        "print(\"// Starting to train Manhattan model............\\n\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator_manhattan.fit(\n",
        "    predictors_manhatten[:training_size_manhattan].values, \n",
        "    targets_manhattan[:training_size_manhattan].values.reshape(training_size_manhattan,NO_TARGETS)/SCALE_NUM_COLS, steps=10000\n",
        ")\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator_manhattan.predict(x=predictors_manhatten[training_size_manhattan:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores'] * SCALE_NUM_COLS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# rmse = np.sqrt(np.mean((shuffle_manhatten['NUM_COLS'][training_size_manhattan:] - predslistscale)**2))\n",
        "rmse = np.sqrt(np.mean((targets_manhattan[training_size_manhattan:].values - predslistscale) ** 2))\n",
        "print('\\n\\n// Lnear Regression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "# Calculate the mean of the NUM_COLS Values.\n",
        "avg = np.mean(shuffle_manhatten['NUM_COLS'][:training_size_manhattan])\n",
        "\n",
        "# Calculate the RMSE using NUM_COLS Values and the mean of all target values.\n",
        "rmse = np.sqrt(np.mean((shuffle_manhatten['NUM_COLS'][training_size_manhattan:] - avg) ** 2))\n",
        "print('\\n\\n// Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb9efcbeb10>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model', '_session_creation_timeout_secs': 7200}\n",
            "// Starting to train Manhattan model............\n",
            "\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model/model.ckpt.\n",
            "INFO:tensorflow:loss = 12774.602, step = 1\n",
            "INFO:tensorflow:global_step/sec: 733.314\n",
            "INFO:tensorflow:loss = 1490.8729, step = 101 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 957.09\n",
            "INFO:tensorflow:loss = 1462.426, step = 201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1003.29\n",
            "INFO:tensorflow:loss = 1645.6594, step = 301 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 973.597\n",
            "INFO:tensorflow:loss = 1570.9368, step = 401 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 996.217\n",
            "INFO:tensorflow:loss = 1488.8328, step = 501 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 967.631\n",
            "INFO:tensorflow:loss = 1401.654, step = 601 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 924.116\n",
            "INFO:tensorflow:loss = 1580.7461, step = 701 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 928.395\n",
            "INFO:tensorflow:loss = 1406.1985, step = 801 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 917.115\n",
            "INFO:tensorflow:loss = 1306.5583, step = 901 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 956.002\n",
            "INFO:tensorflow:loss = 1359.4304, step = 1001 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 903.258\n",
            "INFO:tensorflow:loss = 1813.631, step = 1101 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 933.187\n",
            "INFO:tensorflow:loss = 1548.0771, step = 1201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1006.21\n",
            "INFO:tensorflow:loss = 1465.0924, step = 1301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 966.935\n",
            "INFO:tensorflow:loss = 2601.4004, step = 1401 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1005.5\n",
            "INFO:tensorflow:loss = 1552.9207, step = 1501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 949.195\n",
            "INFO:tensorflow:loss = 1519.467, step = 1601 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 869.949\n",
            "INFO:tensorflow:loss = 1304.9615, step = 1701 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 960.919\n",
            "INFO:tensorflow:loss = 1208.2815, step = 1801 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.872\n",
            "INFO:tensorflow:loss = 1522.3186, step = 1901 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1015.84\n",
            "INFO:tensorflow:loss = 1549.3408, step = 2001 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.088\n",
            "INFO:tensorflow:loss = 1513.8579, step = 2101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1004.85\n",
            "INFO:tensorflow:loss = 2052.176, step = 2201 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1019.59\n",
            "INFO:tensorflow:loss = 1631.7212, step = 2301 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.229\n",
            "INFO:tensorflow:loss = 1419.7534, step = 2401 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 996.447\n",
            "INFO:tensorflow:loss = 1506.166, step = 2501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 881.362\n",
            "INFO:tensorflow:loss = 1770.0363, step = 2601 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.332\n",
            "INFO:tensorflow:loss = 2212.1255, step = 2701 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 954.835\n",
            "INFO:tensorflow:loss = 1513.1326, step = 2801 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 967.314\n",
            "INFO:tensorflow:loss = 1313.7576, step = 2901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 826.886\n",
            "INFO:tensorflow:loss = 1389.1042, step = 3001 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 873.864\n",
            "INFO:tensorflow:loss = 1723.8081, step = 3101 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 846.541\n",
            "INFO:tensorflow:loss = 1585.0273, step = 3201 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 903.548\n",
            "INFO:tensorflow:loss = 1723.9381, step = 3301 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.768\n",
            "INFO:tensorflow:loss = 1167.0629, step = 3401 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 852.724\n",
            "INFO:tensorflow:loss = 1930.2141, step = 3501 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 928.062\n",
            "INFO:tensorflow:loss = 1256.6735, step = 3601 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 947.574\n",
            "INFO:tensorflow:loss = 1529.0066, step = 3701 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.135\n",
            "INFO:tensorflow:loss = 1142.0322, step = 3801 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 931.149\n",
            "INFO:tensorflow:loss = 1482.3247, step = 3901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 911.871\n",
            "INFO:tensorflow:loss = 1317.4397, step = 4001 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 933.416\n",
            "INFO:tensorflow:loss = 1711.1049, step = 4101 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 859.984\n",
            "INFO:tensorflow:loss = 1547.1731, step = 4201 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.02\n",
            "INFO:tensorflow:loss = 1573.194, step = 4301 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 818.015\n",
            "INFO:tensorflow:loss = 1584.2799, step = 4401 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 902.832\n",
            "INFO:tensorflow:loss = 1825.419, step = 4501 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 928.21\n",
            "INFO:tensorflow:loss = 1349.3195, step = 4601 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 965.484\n",
            "INFO:tensorflow:loss = 1544.1233, step = 4701 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 988.102\n",
            "INFO:tensorflow:loss = 1363.3108, step = 4801 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.647\n",
            "INFO:tensorflow:loss = 1797.5393, step = 4901 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 995.508\n",
            "INFO:tensorflow:loss = 2150.35, step = 5001 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.538\n",
            "INFO:tensorflow:loss = 1513.2314, step = 5101 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.612\n",
            "INFO:tensorflow:loss = 1494.4883, step = 5201 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 969.909\n",
            "INFO:tensorflow:loss = 1514.7297, step = 5301 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 926.5\n",
            "INFO:tensorflow:loss = 1503.5758, step = 5401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 974.192\n",
            "INFO:tensorflow:loss = 3282.766, step = 5501 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 881.432\n",
            "INFO:tensorflow:loss = 1840.7224, step = 5601 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 841.397\n",
            "INFO:tensorflow:loss = 1414.3191, step = 5701 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.405\n",
            "INFO:tensorflow:loss = 1905.1532, step = 5801 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 969.62\n",
            "INFO:tensorflow:loss = 1289.4014, step = 5901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 993.621\n",
            "INFO:tensorflow:loss = 1625.9971, step = 6001 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1040.93\n",
            "INFO:tensorflow:loss = 1406.5332, step = 6101 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 952.915\n",
            "INFO:tensorflow:loss = 1262.1644, step = 6201 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 855.141\n",
            "INFO:tensorflow:loss = 1334.5911, step = 6301 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 908.19\n",
            "INFO:tensorflow:loss = 1445.8026, step = 6401 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 902.76\n",
            "INFO:tensorflow:loss = 1598.2233, step = 6501 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 856.869\n",
            "INFO:tensorflow:loss = 1665.1533, step = 6601 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.415\n",
            "INFO:tensorflow:loss = 2597.5352, step = 6701 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.371\n",
            "INFO:tensorflow:loss = 1556.7389, step = 6801 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 949.778\n",
            "INFO:tensorflow:loss = 1453.3818, step = 6901 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 963.849\n",
            "INFO:tensorflow:loss = 1781.894, step = 7001 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 997.615\n",
            "INFO:tensorflow:loss = 1543.4681, step = 7101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 973.012\n",
            "INFO:tensorflow:loss = 1754.2017, step = 7201 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 922.437\n",
            "INFO:tensorflow:loss = 1608.2891, step = 7301 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.918\n",
            "INFO:tensorflow:loss = 1598.983, step = 7401 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 951.596\n",
            "INFO:tensorflow:loss = 1604.8889, step = 7501 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.883\n",
            "INFO:tensorflow:loss = 1467.1013, step = 7601 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 843.515\n",
            "INFO:tensorflow:loss = 1454.4554, step = 7701 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 901.568\n",
            "INFO:tensorflow:loss = 1675.8483, step = 7801 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 967.216\n",
            "INFO:tensorflow:loss = 1459.6841, step = 7901 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.68\n",
            "INFO:tensorflow:loss = 1341.2737, step = 8001 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 938.995\n",
            "INFO:tensorflow:loss = 1099.4469, step = 8101 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 911.696\n",
            "INFO:tensorflow:loss = 1453.4594, step = 8201 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 914.601\n",
            "INFO:tensorflow:loss = 1419.4854, step = 8301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.365\n",
            "INFO:tensorflow:loss = 1840.9343, step = 8401 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 959.872\n",
            "INFO:tensorflow:loss = 1309.71, step = 8501 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 957.036\n",
            "INFO:tensorflow:loss = 1710.1094, step = 8601 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 979.726\n",
            "INFO:tensorflow:loss = 1673.737, step = 8701 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 899.758\n",
            "INFO:tensorflow:loss = 1245.9084, step = 8801 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 1007.93\n",
            "INFO:tensorflow:loss = 1829.1904, step = 8901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 863.006\n",
            "INFO:tensorflow:loss = 1440.8721, step = 9001 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 858.899\n",
            "INFO:tensorflow:loss = 1418.2219, step = 9101 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 922.271\n",
            "INFO:tensorflow:loss = 1494.6128, step = 9201 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 956.728\n",
            "INFO:tensorflow:loss = 1211.9866, step = 9301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 871.457\n",
            "INFO:tensorflow:loss = 1418.6189, step = 9401 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 907.047\n",
            "INFO:tensorflow:loss = 1652.4873, step = 9501 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 967.922\n",
            "INFO:tensorflow:loss = 1938.5557, step = 9601 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 907.277\n",
            "INFO:tensorflow:loss = 1950.3348, step = 9701 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 877.537\n",
            "INFO:tensorflow:loss = 1857.3486, step = 9801 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.215\n",
            "INFO:tensorflow:loss = 1408.1604, step = 9901 (0.105 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1552.1403.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "\n",
            "\n",
            "// Lnear Regression has RMSE of 47.98381144477393\n",
            "\n",
            "\n",
            "// Just using average = 106.14887329146657 has RMSE of 38.29991056088835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjS3zTafVMB8",
        "outputId": "87ffd97d-da39-4574-fd6e-1bb906f8bbf2"
      },
      "source": [
        "# input = pd.DataFrame.from_dict(data = \n",
        "# \t\t\t\t{'DAY' : [1,1,1],\n",
        "#          'MONTH' : [1, 6, 12],\n",
        "#          'YEAR' : [2013, 2016, 2022]\n",
        "#         })\n",
        "\n",
        "input = pd.DataFrame.from_dict(data = \n",
        "\t\t\t\t{'YEAR' : [2013,2016,2022],\n",
        "         'MONTH' : [1, 6, 12],\n",
        "         'DAY' : [1, 1, 1]\n",
        "        })\n",
        "estimator_manhattan = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model', \n",
        "                                                                                 enable_centered_bias=False, \n",
        "                                                                                 feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(input.values)))\n",
        "\n",
        "preds = estimator_manhattan.predict(x=input.values)\n",
        "# print(preds)\n",
        "\n",
        "# The Number of collisions scale will be 106.072969 - which is the average number of collisions in Manhattan (359057 (collisions)/ 3385 (days))\n",
        "predslistnorm = preds['scores']\n",
        "predslistscale = preds['scores'] * 106.072969\n",
        "prednorm = format(str(predslistnorm))\n",
        "pred = format(str(predslistscale))\n",
        "\n",
        "print(prednorm)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb9efcb6cd0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model', '_session_creation_timeout_secs': 7200}\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "[124.43464 133.11383 143.67262]\n",
            "[74660783.38623047 79868298.33984375 86203573.60839844]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hnzo1hOFa-l"
      },
      "source": [
        "### Result of First Run\n",
        "\n",
        "The model is being asked to predict 3 dates.  Two of these dates are known, this will allow how accurate the model is in relateion to real data to be shown.\n",
        "\n",
        "The dates are:\n",
        "\n",
        "1/1/2013  = 882\n",
        "1/6/2016 = 963\n",
        "1/12/2020 = 1061"
      ]
    }
  ]
}