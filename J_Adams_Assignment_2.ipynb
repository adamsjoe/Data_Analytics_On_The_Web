{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "J_Adams_Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUkPRKJw1mm6"
      },
      "source": [
        "# import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# load the data as per normal\n",
        "# df = pd.read_csv('https://raw.githubusercontent.com/adamsjoe/Data_Analytics_On_The_Web/main/Final_Data/Final_Data_Collated.csv', index_col=0)\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/adamsjoe/Data_Analytics_On_The_Web/main/Final_Data/Final_Data_Collated.csv')\n",
        "\n",
        "# now setup to query the dataframe for only the boroughs noted\n",
        "manhat_df = df.query('BOROUGH==\"MANHATTAN\"').head()\n",
        "brooklyn_df = df.query('BOROUGH==\"BROOKLYN\"').head()\n",
        "bronx_df = df.query('BOROUGH==\"BRONX\"').head()\n",
        "queens_df = df.query('BOROUGH==\"QUEENS\"').head()\n",
        "staten_df = df.query('BOROUGH==\"STATEN ISLAND\"').head()\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNzPMAL314nG"
      },
      "source": [
        "# check the data had loaded by printing it\n",
        "print(manhat_df[:6])\n",
        "print(brooklyn_df[:6])\n",
        "print(bronx_df[:6])\n",
        "print(queens_df[:6])\n",
        "print(staten_df[:6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-NlRVds2BEM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "shuffle_manhatten = manhat_df.iloc[np.random.permutation(len(manhat_df))]\n",
        "shuffle_brooklyn = brooklyn_df.iloc[np.random.permutation(len(brooklyn_df))]\n",
        "shuffle_bronx = bronx_df.iloc[np.random.permutation(len(bronx_df))]\n",
        "shuffle_queens = queens_df.iloc[np.random.permutation(len(queens_df))]\n",
        "shuffle_staten_island = staten_df.iloc[np.random.permutation(len(staten_df))]\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T01AcNdrBh3o"
      },
      "source": [
        "# print(manhat_df[:6])\n",
        "# select the day, month, year and number of collisions columns.  No need to worry about the borough as the dataframe contains that borough only\n",
        "predictors_manhatten = shuffle_manhatten.iloc[:,[3,4,5,-1]]\n",
        "predictors_brooklyn = shuffle_brooklyn.iloc[:,[3,4,5,-1]]\n",
        "predictors_bronx = shuffle_bronx.iloc[:,[3,4,5,-1]]\n",
        "predictors_queens = shuffle_queens.iloc[:,[3,4,5,-1]]\n",
        "predictors_staten_island = shuffle_staten_island.iloc[:,[3,4,5,-1]]\n",
        "\n",
        "\n",
        "# print(shuffle[:6])\n",
        "\n",
        "targets_manhattan = shuffle_manhatten.iloc[:,-1]\n",
        "targets_brooklyn = shuffle_brooklyn.iloc[:,-1]\n",
        "targets_bronx = shuffle_bronx.iloc[:,-1]\n",
        "targets_queens = shuffle_queens.iloc[:,-1]\n",
        "targets_staten_island = shuffle_staten_island.iloc[:,-1]\n",
        "\n",
        "# SCALE_NUM_COLS = 1.0\n",
        "\n",
        "# split data into training set\n",
        "training_size_manhattan = int(len(shuffle_manhatten['NUM_COLS']) * 0.8)\n",
        "training_size_brooklyn = int(len(shuffle_brooklyn['NUM_COLS']) * 0.8)\n",
        "training_size_bronx = int(len(shuffle_bronx['NUM_COLS']) * 0.8)\n",
        "training_size_queens = int(len(shuffle_queens['NUM_COLS']) * 0.8)\n",
        "training_size_staten_island = int(len(shuffle_staten_island['NUM_COLS']) * 0.8)\n",
        "\n",
        "# test size is the size of the data - the training size (in this case 20%)\n",
        "testing_size_manhattan = len(shuffle_manhatten['NUM_COLS'] - training_size_manhattan)\n",
        "testing_size_brooklyn = len(shuffle_brooklyn['NUM_COLS'] - training_size_brooklyn)\n",
        "testing_size_bronx = len(shuffle_bronx['NUM_COLS'] - training_size_bronx)\n",
        "testing_size_queens = len(shuffle_queens['NUM_COLS'] - training_size_queens)\n",
        "testing_size_ = len(shuffle_staten_island['NUM_COLS'] - training_size_staten_island)\n",
        "\n",
        "# define the number of input params, day, month and year = 3 (predictors)\n",
        "NO_PREDICTORS_MANHATTAN = 3\n",
        "NO_PREDICTORS_BROOKLYN = 3\n",
        "NO_PREDICTORS_BRONX = 3\n",
        "NO_PREDICTORS_QUEENS = 3\n",
        "NO_PREDICTORS_STATEN_ISLAND = 3\n",
        "\n",
        "# define the number of output params, collisions = 1 (targets)\n",
        "NO_TARGETS = 1\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYOnh5yUElQO"
      },
      "source": [
        "print(predictors_manhatten.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIj_sLztBlsc"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# check tensor version\n",
        "print(tf.__version__)\n",
        "\n",
        "import shutil\n",
        "\n",
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# setup some variables to hold the file path\n",
        "manhattan_dir = '/tmp/linear_regression_trained_model'\n",
        "\n",
        "# remove the last training model\n",
        "shutil.rmtree(manhattan_dir, ignore_errors=True)\n",
        "\n",
        "# estimators for each borough\n",
        "estimator_manhattan = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir=manhattan_dir, hidden_units=[20,18,14], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors_manhatten.values)))\n",
        "\n",
        "# # Prints a log to show model is starting to train\n",
        "print(\"starting to train Manhattan model\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator_manhattan.fit(\n",
        "    predictors[:training_size_manhattan].values, \n",
        "    targets[:training_size_manhattan].values.reshape(training_size_manhattan, \n",
        "    NO_TARGETS)/SCALE_NUM_COLS, \n",
        "    steps=10000\n",
        ")\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator_manhattan.predict(x=predictors_manhatten[training_size_manhattan:].values)\n",
        "\n",
        "# # Apply the Scale value (not really needed here) to the outputs.\n",
        "# predslistscale = preds['scores']*SCALE_NUM_TRIPS\n",
        "\n",
        "# # pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# # Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# # i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# # find the average of all the squares and then find the square root. \n",
        "# # The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "# rmse = np.sqrt(np.mean((targets[training_size:].values - predslistscale)**2))\n",
        "# print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# # Calculate the mean of the Life Satisfaction Values.\n",
        "# avg = np.mean(shuffle['NUM_TRIPS'][:training_size])\n",
        "\n",
        "# # Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# # The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# # In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "# rmse = np.sqrt(np.mean((shuffle['NUM_TRIPS'][training_size:] - avg)**2))\n",
        "# print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}